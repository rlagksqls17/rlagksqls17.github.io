---  
title: "기계학습 정리 2장 - 선형대수"
excerpt: "last_modified_at: 2021-05-04"
categories:
  - Blog
layout: post
tags:
  - Blog
use_math: True
katex: True
toc: True
toc_sticky: False
toc_label: "페이지 주요 목차"
last_modified_at: 2021-05-03T08:19:00~21:00  
---  

[![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Frlagksqls17.github.io%2Fblog%2FMachLearn2%2F&count_bg=%2379C83D&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false)](https://hits.seeyoufarm.com)  


기계학습은 다음 서적을 참고하여 정리하였다. 책에 있는 모든 내용이 있는게 아니니 자세한 내용은 책을 참고하길 바란다.

![KakaoTalk_20210430_231147826](https://imgur.com/yrLo7lR.jpg)  

> 책에 있는 객관적인 내용은 정자로, 주관적으로 풀어쓴 의견은 기울임체로  작성하였다.
> 중요하다고 생각되는 내용은 **굵은 글자**로 작성하였다.  

---  



# 2장 - 기계학습과 수학  

### 1. 선형대수  

#### - 벡터와 행렬  

기계 학습에서는 입력된 샘플을 특징 벡터로 표현한다. 예를 들어 {5.1 3.5, 1.4, 0.2}의 4개의 특징으로 정의되는 샘플은 다음과 같이 네 수를 수직 방향으로 나열하여 표현 가능하다.  

$\mathbf{x} =$  
$$\begin{pmatrix}
x_1 \\
x_2 \\
x_3 \\
x_4
\end{pmatrix}$$
$=$  
$$\begin{pmatrix}
5.1 \\
3.5 \\
1.4 \\
0.2
\end{pmatrix}$$  

이것을 **벡터(vector)** 라 하며 굵은 글씨의 소문자 기호로 표기한다.  
이 벡터는 4개의 요소로 구성되기 때문에, 4차원이라고 한다.  

**행렬(matrix)** 은 아래 식과 같이 여러 개의 벡터를 담을 수 있다.  

$\mathbf{X} =$   
$$\begin{pmatrix}
5.1 & 5.2 & 5.3 \\
3.5 & 3.6 & 3.7\\
1.4 & 1.5 & 1.6 \\
0.2 & 0.3 & 0.4
\end{pmatrix}$$  

위 예는 3개의 벡터를 담음 행렬인데, 가로 방향을 행, 세로 방향을 열이라고 한다.  
기계 학습에서는 훈련집합을 담은 행렬을 설계행렬 (design matrix)라고 한다.  

---  
##### 행렬의 종류  

전치행렬  
$\mathbf{A^T}$ 로 표현한다. 행과 열을 교환한 형태로,  
- $\mathbf{A} =$  
$$\begin{pmatrix}
3 & 4 & 1 \\
0 & 5 & 1
\end{pmatrix}$$  
이라면  
$\mathbf{A^T} =$  
$$\begin{pmatrix}
3 & 0 \\
4 & 5 \\
5 & 1
\end{pmatrix}$$  
로 표현한다.  

정사각행렬  
행과 열의 개수가 같은 행렬을 말한다.  
$$\begin{pmatrix}
2 & 0 & 1 \\
1 & 21 & 5 \\
4 & 5 & 12
\end{pmatrix}$$    

대각행렬  
주 대각선을 제외한 요소가 모두 0인 행렬을 말한다.  
$$\begin{pmatrix}
50 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 12
\end{pmatrix}$$    

단위행렬  
주 대각선 요소가 1이고 나머지 요소가 모두 0인 정사각행렬을 말한다.  
$$\begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{pmatrix}$$  

대칭행렬  
$$\begin{pmatrix}
1 & 2 & 11 \\
2 & 21 & 5 \\
11 & 5 & 1
\end{pmatrix}$$  


---  



##### 행렬의 연산  

- 덧셈 : 해당하는 요소끼리 더하면 된다.  
$$\begin{pmatrix}
1 & 2 \\
2 & 5
\end{pmatrix}$$  
+  
$$\begin{pmatrix}
2 & 1 \\
2 & 6
\end{pmatrix}$$  
=  
$$\begin{pmatrix}
3 & 3 \\
4 & 11
\end{pmatrix}$$  

- 곱셈 : *곱셈의 경우 아래 식을 참고하여 이해하길 바란다.*  
$행렬 \mathbf{A}$ 의 열의 개수와 $행렬 \mathbf{B}$ 의 행의 개수가 같을 때만  
$행렬 \mathbf{C} = {\mathbf{AB}}, c_{ij} = \sum_{k=1,s} a_{ik} b_{kj}$  

- 내적(dot product) : 차원이 같은 두 벡터 a와 b의 곱을 말한다.  
내적 $\mathbf{a} \cdot  \mathbf{b} = \mathbf{a^T} \mathbf{b} = \sum_{k=1,d} a_{k} b_{k}$  

---  
##### 텐서 (tensor)
기계 학습에서는 3차원 이상의 숫자 배열이 필요한 경우가 종종 있다. 예를 들어, RGB 영상을 입력받는 경우 2차원 행렬이 세 장 있는 셈이다.  

![Imgur](https://i.imgur.com/yR8UR94.jpg)  
[출처 링크](https://ko.wikipedia.org/wiki/%ED%85%90%EC%84%9C)  

---  
#### - 놈과 유사도  

##### 놈  
**벡터의 크기를 정의** 할 때는 놈(norm)을 사용한다.  

식은 다음과 같고, p차 놈이라고 한다.  

* $\|\|$  $\mathbf{x}_p$  $\|\|$  $= (\sum_{i = 1, d} \|x_i\|^p)^{1 \over p}$  

이때 **2차 놈을 유클리디언 놈 또는 L2놈** 이라 하고, 가장 널리 쓰인다는 이유로 $\|\|\mathbf{x}\|\|_2$ 대신 $\|\|\mathbf{x}\|\|$ 로 종종 표기한다.  

**놈을 이용한 법칙으로, $행렬 \mathbf{x} = \big ( \begin {smallmatrix} 3 & 4 & 1 \end {smallmatrix} \big ) $ 일 때 이 행렬의 각 요소를 행렬 x의 2차 놈으로 나눠주면 단위벡터로 만들어줄 수 있다.**  

* $단위벡터 : {\mathbf{x} \over \|\|\mathbf{x} \|\| }_2 $

기계 학습이 주로 사용하는 프로베니우스 놈의 경우 각 행렬 내의 요소들의 제곱합의 제곱근을 구해주면 된다.  

---  

##### 유사도와 거리  

기계 학습이 사용하는 중요한 연산 중 하나는 두 샘플의 유사도측정이다. 유사도를 이용하면 비슷한 샘플을 찾아 같은 군집으로 모으거나, 가장 유사한 샘플을 찾아 그 샘플이 속한 부류로 분류할 수 있다.  


![Imgur](https://i.imgur.com/QJwqshO.jpg)  

참고한 책의 그림 2-2이다.  

> 그림과 같이 좌표 상에 세 벡터를 표시했을 때, 벡터 간의 유사도를 측정하는 문제를 풀려하면 주로 내적을 이용한다.   

내적을 구할 경우 벡터의 방향이 달라질수록 값이 작아지므로 유사도 측정에 사용가능하다.  **하지만 벡터가 길기만 하면 유사도가 커지는 문제가 있다.**  위 그림에서 x와 x1의 내적은 14이고, x와 x2의 내적은 18이므로 x는 x2와 더 유사하다고 말해야하는 오류가 생긴다.  

**코사인 유사도** 를 사용하면 위 문제를 해결 가능하다. 먼저 위 벡터를 단위벡터로 변환한 다음, 단위 벡터의 내적을 사용하면 된다.  

* $cosinesimilarity(\mathbf{a,b}) = {\mathbf{a} \over \|\|\mathbf{a}\|\|} \cdot {\mathbf{b} \over \|\|\mathbf{b}\|\|} = cos(\theta)$  

이렇게 되면 x와 x1의 코사인 유사도는 0.8682, x와 x2의 코사인 유사도는 0.7474가 되어 x1이 더 유사함을 확인할 수 있다.  

---  

#### 퍼셉트론의 해석  

**퍼셉트론은 입력 샘플을 2개의 부류 중 하나로 분류하는 분류기이다.**  

![Imgur](https://i.imgur.com/5uJmH4g.jpg)  

위 그림은 퍼셉트론의 구조 (책 2-3 (a))이다.  
왼쪽에는 특징 벡터 x를 입력할 수 있도록 d개의 입력 노드가 있고,  
오른쪽에는 출력 노드가 1개 있는데 노드의 출력값을 *o* 로 표기한다.  
입력 노드와 출력 노드는 선(에지)으로 연결되어 있는데, 에지마다 가중치(w)가 부여된다.  

> 여기서 살짝 헷갈릴 수 있다. 이 그림에서 d의 뜻은, **행렬의 갯수가 아니라 행렬 내의 요소 갯수** 이다. 예를 들어 $\mathbf{x} = \{x_1, x_2, x_3, x_4 \}$ 에서, 각 요소들을 각각의 입력 노드에 집어 넣는 것과 같다.  

$\mathbf{w} \cdot \mathbf{x}$ 는 입력 노드의 값과 해당 에지의 가중치를 곱하여 더한 값인데, 이 값을 **활성값(activation value)** 이라고 한다. 이 활성값을 활성함수 r에 넣었을 때 출력되는 값이 *o* 가 된다.  

* $o = r(\mathbf{w \cdot x}) = \begin{cases} 1, \{if a>=T\} \\ -1, \{if a <T\}\end{cases}$  

위 식은 그림처럼 좌표와 결정직선으로도 표현 가능하다. 이 그림에서 파란 직선은 전체공간을 +1인 부분공간과 -1인 부분공간으로 구분하기 때문에 결정직선이라고 한다. 결정직선 위쪽, 즉 +1로 표시된 영역에 속한 점은 $\mathbf{w \cdot x >} T$ 에 속하는 부분공간이고, 결정직선 아래쪽, 즉 -1로 표시된 영역에 속한 점은 $\mathbf{w \cdot x <} T$이다.

> 즉 분류기라는 퍼셉트론의 특성을 다시금 느낄 수 있다.  

![Imgur](https://i.imgur.com/i18j2lG.jpg)  
책 (2-3 (c))

위 그림은 2차원 공간이기 때문에 결정 **직선** 이 공간을 분할하였는데, 3차원이 되면 결정 **평면** 이 분할을 담당한다. 4차원 이상에서는 이를 **결정 초평면**이라고 한다.  

여러 개의 퍼셉트론을 묶어 다음 그림과 같은 구조로 확장할 수 있다.  

![Imgur](https://i.imgur.com/pwCRBqr.jpg)  
책 (2-5)

이제 가중치 첨자를 2개 사용하는데, 출력노드가 여러개이므로 출력 o는 벡터로 표기해야 한다. 또한 i번째 입력 노드와 j번째 출력 노드를 잇는 에지 가중치를 $w_{ji}$ 라 하면, j번째 퍼셉트론의 가중치 벡터를 $\mathbf{w_j} =  (w_{j1}, w_{j2}, \cdots, w_{jd} )$ 라 할 수 있다. 즉 다음 식과 같이 행렬을 사용하여 간결하게 표현할 수 있다. **이처럼 선형대수는 기계 학습에서 발생하는 연산을 간결하게 표현하는 데 매우 효과적이다.**  

> 전의 식과 다르게 r과 o를 단순한 값이 아닌 벡터로 표기 하였다는 것을 참고하고, 왜 이렇게 표기가 다른 것인지 확인해보면 선형대수를 이해하는데 도움 될 것이다.

$\mathbf{o} = \mathbf{r} \begin {pmatrix} \mathbf{w_1 \cdot x} \\ \mathbf{w_2 \cdot x} \\ \cdots \\ \mathbf{w_c \cdot x}  \end {pmatrix} $  $= \mathbf{r(Wx)}$  

---  
##### 학습의 정의  

> 이렇게 학습을 마친 퍼셉트론은, 가중치와 샘플을 알고 있는 과정에서, 각 샘플들을 분류하는 과업을 수행한다.  

$\mathbf{o} = \mathbf{r(Wx)}$ (이때 출력값을 분류하는 과정)  

하지만 이런 프로그램을 제작하기 위해서는 훈련집합을 이용하여 학습이라는 과정을 성공적으로 마쳐야만 한다.  

> 퍼셉트론을 예로 들었을 때 기계 학습이란 특징 벡터 x와 부류 정보 o의 쌍이 주어졌을 때 샘플을 제대로 분류하는 W를 구하는 문제이다.  

$\mathbf{o} = \mathbf{r(Wx)}$ (이때 최적의 가중치를 찾아내는 과정)  

여기서 하나의 샘플 뿐만이 아니라 모든 샘플을 만족하는 최적의 가중치를 구해야만 한다.

> 이러한 상황에서 가중치의 최적해를 구하려면 최적화 알고리즘을 동원해야 하는데, 이는 추후에 기술하도록 하겠다.   

---  

#### 선형결합과 벡터공간  

벡터 a와 b에 각각 상수를 곱한 다음 결과를 더하는 연산을 **선형결합**이라고 한다.  

$\mathbf{c} = a_1\mathbf{a} + a_2\mathbf{b}$  

위 식에서 $a_1$ 과 $a_2$를 조금씨 변화시키면서 만들어지는 벡터 c를 이차원 공간에 표시했을 때, 이렇게 만들어지는 공간, 즉 선형결합으로 만들어지는 공간을 **벡터공간**이라고 하고, 벡터공간을 만드는 2개의 벡터 a와 b를 기저 벡터라고 한다.    

다음 그림은 기저 벡터와 벡터 공간을 표시한다.  

![Imgur](https://i.imgur.com/DNTUXY2.jpg)  

* 한 기저 벡터를 나머지 기저 벡터의 선형결합으로 만들 수 없을 때, 두 기저 벡터가 **선형 독립**이라고 하고, 벡터공간은 직선으로밖에 나타나지 않는다.  

컴퓨터 프로그래밍을 할 시에는 행렬로 벡터공간을 분석해야하는데, 이 때 가장 도움 되는 것이 **계수(rank)** 다. 계수를 알면 벡터공간이 어떻게 생겼는지 알 수 있다.  

> 다음을 통해 계수의 개념을 짐작해보자.  


$$\begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{pmatrix}$$  

: 선형 독립 행렬 3개, 계수 3  (벡터 공간은 3차원 공간 전체)  

$$\begin{pmatrix}
1 & 0 & 0 \\
0.8 & 1 & 0.8 \\
0 & 0 & 1
\end{pmatrix}$$  

: 선형 독립 행렬 2개, 계수 2 (벡터 공간은 평면)  

$$\begin{pmatrix}
0 & 0 & 4 \\
0 & 0 & 0.1 \\
0 & 0 & 1.2  
\end{pmatrix}$$  

: 선형 독립 행렬 1개, 계수 1 (벡터 공간은 직선)  

---  


#### 역행렬  

정사각행렬 A에 대해 다음 식을 만족하는 행렬 $\mathbf{A^{-1}}$ 을 $\mathbf{A}$ 의 역행렬이라 한다.  

* $\mathbf{A^{-1}}\mathbf{A} = \mathbf{A}\mathbf{A^{-1}} = \mathbf{I}$

>여기서 $\mathbf{I}$는 단위행렬이다.  

한편 역행렬이 없는 행렬도 있는데, 이를 특이행렬이라 한다.  

다음 성질은 모두 역행렬에 관련한 필요충분조건이다.  
* A는 역행렬을 가진다. 즉, 특이행렬이 아니다.  
* A는 최대계수를 가진다.  
* A의 모든 행이 선형독립이다.  
* A의 모든 열이 선형독립이다.  
* A의 행렬식은 0이 아니다.  
* $\mathbf{A^T}\mathbf{A}$ 는 양의 정부호 대칭 행렬이다.  
* A의 고유값은 모두 0이 아니다.  

---  

##### 행렬식  

> 고등학교때 배웠을수도 있겠다. 행렬식을 이용하면 역행렬을 쉽게 구할 수 있다.  

$\mathbf{A^{-1}} = \big ( \begin {smallmatrix} a & b \\ c & d \end {smallmatrix} \big )^{-1} = {1 \over ad -bc}\big ( \begin {smallmatrix} d & -b \\ -c & a \end {smallmatrix} \big )$  

---  
##### 정부호 행렬  

* 양의 정부호 행렬 : 0 이 아닌 모든 벡터 $\mathbf{x}$ 에 대해, $\mathbf{x^T}\mathbf{A}\mathbf{x} > 0$  

* 양의 준정부호 행렬 : 0 이 아닌 모든 벡터 $\mathbf{x}$ 에 대해, $\mathbf{x^T}\mathbf{A}\mathbf{x} >= 0$  

* 음의 정부호 행렬 : 0 이 아닌 모든 벡터 $\mathbf{x}$ 에 대해, $\mathbf{x^T}\mathbf{A}\mathbf{x} < 0$

* 음의 준정부호 행렬 : 0 이 아닌 모든 벡터 $\mathbf{x}$ 에 대해, $\mathbf{x^T}\mathbf{A}\mathbf{x} <= 0$  

부정부호 행렬은 벡터에 따라 음수가 되기도 하고 양수가 되기도 한다.  

---  
#### 행렬 분해  

##### 고유값과 고유벡터  

다음 식을 만족하는 0이 아닌 벡터 v를 행렬 A의 고유벡터라고 하며, 고유 벡터에 대응하는 실수 $\lambda$ 를 고유값이라고 한다.  

> 나중에 코딩할 때 써먹을 수 있으니 지금 이해해두는게 좋다.

* $\mathbf{Av} = \lambda\mathbf{v}$  

##### 고유값 분해 (Eigen Value Decomposition)  
* $\mathbf{A} = \mathbf{Q\Lambda Q}^{-1}$  

$\mathbf{Q}$ 는 $\mathbf{A}$의 고유 벡터를 열에 배치한 행렬이고 \Lambda는 고윳값을 대각선에 배치한 대각행렬이다.  

> 이때 고윳값 분해는 정사각행렬이 아니면 적용할 수 없다는 것을 유의하자.  

##### 특잇값 분해 (Singular Value Decomposition)  
* $\mathbf{A} = \mathbf{U \Sigma V^T}$  

줄여서 SVD라고도 한다. U를 왼쪽 특이 행렬이라고 하는데, $\mathbf{AA^T}$ 의 고유 벡터를 열에 배치한 n * n 행렬이다. V는 오른쪽 특이행렬로, $\mathbf{A^T A}$ 의 고유 벡터를 열에 배치한 m\*m 행렬이다.  $\Sigma$ 는  $\mathbf{A^T A}$의 고윳값의 제곱근을 대각선에 배치한 대각행렬로서 n\*m크기이다.    

---  

다음 포스트에서는 이어서 기계학습에 필요한 기초 확률과 통계를 작성하도록 하겠다.  
